{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLG454E - Learning From Data, Homework 3\n",
    "\n",
    "In this homework, you are supposed to implement following parts:\n",
    "     \n",
    "- **Part 1: solve an SVM optimization problem by hand (50 points)**\n",
    "\n",
    "- **Part 2: practice feature selection (50 points)**\n",
    "     - Refer to Machine Learning Blinks 10 and 11 for this part:\n",
    "     - ML Blinks 10: https://www.youtube.com/watch?v=laeth5oT9YM&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ\n",
    "     - ML Blinks 11: https://www.youtube.com/watch?v=mRmVKNklE9I&list=PLug43ldmRSo1LDlvQOPzgoJ6wKnfmzimQ \n",
    " \n",
    " \n",
    " \n",
    " ### Important Notes:\n",
    "   - Please complete this template and include any other necessary materials (screenshots of your handwritten solutions etc.) into the HW3 folder. Then zip it again and submit to Ninova.\n",
    "   - At Part 1, you can upload the screenshots of your handwritten solutions to the Notebook. But please be sure that your solutions are neat and can be read properly.\n",
    "   - For the Part 2, you should implement feature selection from scratch however can use scikit-learn built-in functions for training the SVM.\n",
    "   - You can ask your questions on Ninova message board or send an e-mail to akti15@itu.edu.tr.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Solving SVM optimization by hand (50 points)\n",
    "\n",
    "You can insert the screenshots of your handwritten solution on Jupyter Notebook. For an example, check the cells including images. Do not forget to include your solution image file into the submitted .zip file.\n",
    "\n",
    "Some reminders for the question:\n",
    "\n",
    " - Lagrangian to optimize: $\\mathcal{L}_{primal} = \\sum_{i=1}^{n} a_{i} - \\frac{1}{2} [\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x^{j}] $ \n",
    "\n",
    "\n",
    "- Constraint: $\\sum_{i=1}^{n} \\alpha_{i} y_{i} = 0$\n",
    "\n",
    "\n",
    "- Optimal parameter: $w^{*} = \\sum_{i=1}^{n} \\alpha_{i} y_{i} x^{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 (25 points)\n",
    "\n",
    "<p style=\"float: left;\"><img src=\"Part1-1.png\" width = \"200\"></p>\n",
    "        \n",
    "        Given the two following training samples (n=2), provide below a step-by-step solution\n",
    "        to estimate the optimal parameters (w and b) of the hyperplane separating the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 (15 points) \n",
    "If we add a third training point $x_3 = \\left [\\begin{matrix} -4 \\\\ 4 \\end{matrix}\\right] $, will that impact the hyperplane estimated using points $x_1$ and $x_2$? Answer this considering two cases where label for $x_3$ is (1) $y_3 = +1 $, (2) $y_3 = -1 $ and justify. You do not need to solve Lagrangian again, justify your answer drawing the hyperplanes and giving explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3 (10 points)\n",
    "Explain how to classify the point $x_{test} = \\left [\\begin{matrix} -4 \\\\ -3 \\end{matrix}\\right] $ using the estimated model at Part 1.1. What is the predicted label of $x_{test}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3 Solution\n",
    "\n",
    "*Double click here to insert your solution.*\n",
    "<img src=\"\" width = \"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature selection using Variance Threshold (50 points)\n",
    "\n",
    "In this part, we will use scikit-learn library for training SVM. You can install the necessary package using following commands:\n",
    "\n",
    "        > python3 -m pip install scikit-learn\n",
    "        > conda install -c conda-forge scikit-learn\n",
    "        \n",
    "There are lots of machine learning techniques that are available in scikit-learn library. In this problem we will use the **SVM** classifier with linear kernel and implement Variance Threshold feature selection method from scratch.\n",
    "   \n",
    "\n",
    "You can check the documentations on the internet to learn how to use SVM classifier and which parameters to use. Necessary functions are imported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is associated with the book\n",
    "# \"Machine Learning Refined\", Cambridge University Press, 2016.\n",
    "# by Jeremy Watt, Reza Borhani, and Aggelos Katsaggelos.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "import sklearn.datasets as ds\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement feature selection on handwritten digits dataset. In the following cell we load and examine dataset. As you can see the samples are 8 by 8 images where in each row of X, the images are kept as flatten vectors. Each feature represents a pixel (i.e 0th feature is (0, 0) pixel and 8th feature is (1, 0) pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_data = ds.load_digits()\n",
    "print(\"Number of samples: \", digit_data.data.shape[0])\n",
    "print(\"Number of attributes: \", digit_data.data.shape[1])\n",
    "print(\"Classes: \", digit_data.target_names)\n",
    "\n",
    "c = digit_data.images[0]\n",
    "for i in range(1, 10):\n",
    "    c = np.concatenate((c, digit_data.images[i]), 1)\n",
    "\n",
    "plt.gray() \n",
    "plt.matshow(c)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data into training and testing sets in order to train the models on training set and test them on unseen test set. Do not change the random state so we will get the same train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = digit_data.data, digit_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle = True, random_state=20)\n",
    "\n",
    "print(\"Number of training samples: \", X_train.shape[0])\n",
    "print(\"Number of testing samples: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(15 points)** In the following cell, you should complete the necessary train and test functions for SVM. Fill the specified parts only. Calculate the accuracy score as:\n",
    "\n",
    "<br>\n",
    "<center> $ Accuracy = \\frac{\\#correct \\ predictions}{\\#total \\ samples} $ </center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y):\n",
    "    \n",
    "    classifier = ...  # define the classifier\n",
    "    classifier = ...  # fit the classifier on training set\n",
    "    \n",
    "    preds = ...  # predict the labels for training set\n",
    "    \n",
    "    train_accuracy = ...  # calculate the accuracy (do not use built-in function)\n",
    "    \n",
    "    return classifier, train_accuracy\n",
    "\n",
    "def test(classifier, X, y):\n",
    "    \n",
    "    test_accuracy = ...  # calculate the accuracy on test set\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5 points)** Next, use these function to train and test an SVM without feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM classifier without feature selection\n",
    "svm, train_acc = ... # call train function with necessary parameters\n",
    "test_acc = ...  # call test function with necessary parameters\n",
    "\n",
    "print(\"Train acc without feature selection: \", train_acc)\n",
    "print(\"Test acc without feature selection: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(20 points)** Now, implement Variance Threshold feature selection method on the dataset. The Variance Threshold basicly follows these steps:\n",
    "  - Calculate the variances of each feature in the dataset. (Hint: You can use numpy for this)\n",
    "  - Eliminate the features that has variance less then the given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: YOUR CODE GOES HERE##\n",
    "\n",
    "def variance_threshold(X, threshold=0):\n",
    "    \n",
    "    selected = None # keep the indexes of selected feature\n",
    "    eliminated = None # keep the indexes of eliminated features\n",
    "    X_reduced = None # X with reduced feature set\n",
    "    \n",
    "    #calculate variances of each feature and eliminate features with variance lower than threshold.\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Indexes of the eliminated features: \", eliminated) # print the indexes of eliminated features.\n",
    "    \n",
    "    print(\"Number of features for threshold = \", threshold, \"is :\", X_reduced.shape[1])\n",
    "    return X_reduced, selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points)** Now, call the variance threshold function for **5** distinct threshold values and obtain **different set of features**. Then, train and test an SVM classifier on each set. Do not forget to apply the feature selection on the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: YOUR CODE GOES HERE##\n",
    "\n",
    "thresholds = [...]\n",
    "\n",
    "for th in thresholds:\n",
    "    print(\"Threshold = \", th)\n",
    "    X_reduced, selected = variance_threshold(...)\n",
    "    X_reduced_test = ...\n",
    "\n",
    "    svm, train_acc = ... # call train function with necessary parameters\n",
    "    test_acc = ...  # call test function with necessary parameters\n",
    "\n",
    "    print(\"Train acc with feature selection: \", train_acc)\n",
    "    print(\"Test acc with feature selection: \", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Question (5 points)**\n",
    "Why the model performance varies with the number of selected features? Justify your answer with a toy example (i.e., using a simulated dataset). You can also refer to the results you got in Part 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Double click here to insert your answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
